{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "MKsRDH5ZUdfasdv"
   },
   "source": [
    "# Capstone project EDA\n",
    "\n",
    "### By Victor Ramirez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7X58hOMTUH-w"
   },
   "outputs": [],
   "source": [
    "# Import the libraries we'll use below.\n",
    "\n",
    "# standard\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "# tf and keras\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from sklearn import preprocessing\n",
    "from tensorflow import keras\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "# plots\n",
    "import seaborn as sns  # for nicer plots\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set(style=\"darkgrid\")  # default style\n",
    "import plotly.express as px\n",
    "\n",
    "from scipy import stats # For in-built method to get PCC\n",
    "\n",
    "random.seed(2)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rHLcriKWLRe4"
   },
   "source": [
    "## Understanding the data\n",
    "\n",
    "Before doing any training (or evaluating), let's make sure we understand what we're working with. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "load_auto_data_set_text"
   },
   "source": [
    "### Load the data\n",
    "\n",
    "We'll only use a few of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from a CSV file into a pandas dataframe. Remember that each row\n",
    "# is an example and each column in a feature.\n",
    "fbi_data = pd.read_csv(\n",
    "    'F:/code/uc berkeley/mids-210/datasciw210/EDA/fbi_hatecrime_census_bea.csv',\n",
    "    sep=',', encoding='latin-1')\n",
    "\n",
    "sf_data = pd.read_csv(\n",
    "    'F:/code/uc berkeley/mids-210/datasciw210/EDA/Police_Department_Incident_Reports__2018_to_Present.csv'\n",
    ")\n",
    "\n",
    "# ca_data = pd.read_csv(\n",
    "#     'F:/code/uc berkeley/mids-210/datasciw210/EDA/hci_crime_752_pl_co_re_ca_2000-2013_21oct15-ada.xlsx'\n",
    "# )\n",
    "\n",
    "# sf_data.drop(columns=['location'], inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data EDA\n",
    "print(sf_data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(sf_data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data cleaning\n",
    "\n",
    "# check for null values\n",
    "print(sf_data.isnull().sum())\n",
    "\n",
    "columns = sf_data.columns\n",
    "##################\n",
    "### DROP zeros ###\n",
    "##################\n",
    "for col in columns:\n",
    "    if (sf_data[col] == 0).all():\n",
    "        print('All values in ', col ,' column are zero dropping col')\n",
    "        del sf_data[col]\n",
    "    else:\n",
    "        print('All values in ', col ,' column are not zero')\n",
    "    \n",
    "\n",
    "print('Number of features: %s' %sf_data.shape[1])\n",
    "print('Number of examples: %s' %sf_data.shape[0])\n",
    "\n",
    "# print('Number of location', len(pd.unique(sf_data['location'])))\n",
    "\n",
    "# Find unique values of a column\n",
    "# print(sf_data['location'].unique())\n",
    "\n",
    "print(sf_data.columns.tolist())\n",
    "\n",
    "print('Columns in my data: ', sf_data.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data cleaning\n",
    "\n",
    "# check for null values\n",
    "print(fbi_data.isnull().sum())\n",
    "\n",
    "columns = fbi_data.columns\n",
    "##################\n",
    "### DROP zeros ###\n",
    "##################\n",
    "for col in columns:\n",
    "    if (fbi_data[col] == 0).all():\n",
    "        print('All values in ', col ,' column are zero dropping col')\n",
    "        del fbi_data[col]\n",
    "    else:\n",
    "        print('All values in ', col ,' column are not zero')\n",
    "    \n",
    "\n",
    "print('Number of features: %s' %fbi_data.shape[1])\n",
    "print('Number of examples: %s' %fbi_data.shape[0])\n",
    "\n",
    "print('Number of location', len(pd.unique(fbi_data['location'])))\n",
    "\n",
    "# Find unique values of a column\n",
    "print(fbi_data['location'].unique())\n",
    "\n",
    "print(fbi_data.columns.tolist())\n",
    "\n",
    "print('Columns in my data: ', fbi_data.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle order\n",
    "indices = np.arange(hc.shape[0])\n",
    "print('indices:', indices, '\\n')\n",
    "\n",
    "# set seed\n",
    "np.random.seed(0)\n",
    "shuffled_indices = np.random.permutation(indices)\n",
    "print('shuffled indices:', shuffled_indices, '\\n')\n",
    "\n",
    "# reindex to change the ordering of the original\n",
    "hc = hc.reindex(shuffled_indices)\n",
    "display(hc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visual hate_crime_incident_count summary in a treemap plot\n",
    "\n",
    "# fig = px.treemap(fbi_data, path=[px.Constant(\"hate_crime_incident_count EDA\"), 'location', 'year', 'month'], \n",
    "                 # values='hate_crime_incident_count')\n",
    "# fig.update_layout(margin = dict(t=50, l=25, r=25, b=25))\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visual EDA \n",
    "\n",
    "#plot the histogram of hate_crime_incident_count to see the distribution of the point data\n",
    "sns.displot(fbi_data, x=\"hate_crime_incident_count\", height=8, aspect=15/8)\n",
    "\n",
    "#plot the histogram of all features to see the distribution of the point data\n",
    "fbi_data.hist(figsize=(36, 30), bins=50, xlabelsize=8, ylabelsize=8);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation EDA\n",
    "core_value = 0.83\n",
    "\n",
    "df_num_corr = bea.corr()['hate_crime_incident_count']\n",
    "# create a golden list of high correlation features values\n",
    "golden_features_list = df_num_corr[abs(df_num_corr) > core_value].sort_values(ascending=False)\n",
    "print(\"There are {} strongly correlated values with a greater than 0.83 correlation with the hate_crime_incident_count:\\n{}\".format(len(golden_features_list), golden_features_list))\n",
    "\n",
    "# create a golden list of high correlation features labels\n",
    "quantitative_features_list = ['72_capital_outlay', '80_other_public_welfare', '18_individual_income', '71_higher_education', '77_public_welfare', '101_housing_and_community_development', '83_health',\n",
    "    '3_intergovernmental_revenue', '4_from_federal_government', '96_protective_inspection_and_regulation', '114_capital_outlay', '61_assistance_and_subsidies', '70_capital_outlay', '107_judicial_and_legal',\n",
    "    '63_insurance_benefits_and_repayments', '120_insurance_trust_expenditure', '113_utility_expenditure', '79_vendor_payments']\n",
    "\n",
    "# By convention, when the\n",
    "# p-value is  <  0.001: we say there is strong evidence that the correlation is significant.\n",
    "# the p-value is  <  0.05: there is moderate evidence that the correlation is significant.\n",
    "# the p-value is  <  0.1: there is weak evidence that the correlation is significant.\n",
    "# the p-value is  >  0.1: there is no evidence that the correlation is significant.\n",
    "\n",
    "for x in quantitative_features_list:\n",
    "    pearson_coef, p_value = stats.pearsonr(bea[x], bea['hate_crime_incident_count'])\n",
    "    print(\"The Pearson Correlation Coefficient is\", pearson_coef, \" with a P-value of P =\", p_value)      \n",
    "    \n",
    "    \n",
    "# we will use these input features\n",
    "features = []\n",
    "\n",
    "for i in range(0,len(hc.columns)):\n",
    "  feature_add = hc.columns[i]\n",
    "  features.append(feature_add)\n",
    "\n",
    "features.remove('hate_crime_incident_count')\n",
    "print(features)\n",
    "len(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fbi_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more EDA \n",
    "# display all the correlated values to find outliers\n",
    "\n",
    "for i in range(0, len(fbi_data.columns), 5):\n",
    "    sns.pairplot(data=fbi_data,\n",
    "                x_vars=fbi_data.columns[i:i+5],\n",
    "                y_vars=['hate_crime_incident_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_auto_data_set_code"
   },
   "outputs": [],
   "source": [
    "# more correlation EDA \n",
    "\n",
    "corr = fbi_data.drop('hate_crime_incident_count', axis=1).corr() \n",
    "plt.figure(figsize=(50, 50))\n",
    "\n",
    "# viridis color\n",
    "sns.heatmap(corr[(corr >= 0.5) | (corr <= -0.4)], \n",
    "            cmap='viridis', vmax=1.0, vmin=-1.0, linewidths=0.1,\n",
    "            annot=True, annot_kws={\"size\": 8}, square=True);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  quantitative features of our dataframe and how they relate to the hate_crime_incident_count\n",
    "quantitative_features_list = ['72_capital_outlay', '80_other_public_welfare', '18_individual_income', '71_higher_education', '77_public_welfare', '101_housing_and_community_development', '83_health',\n",
    "    '3_intergovernmental_revenue', '4_from_federal_government', '96_protective_inspection_and_regulation', '114_capital_outlay', '61_assistance_and_subsidies', '70_capital_outlay', '107_judicial_and_legal',\n",
    "    '63_insurance_benefits_and_repayments', '120_insurance_trust_expenditure', '113_utility_expenditure', '79_vendor_payments']\n",
    "\n",
    "df_quantitative_values = fbi_data[quantitative_features_list]\n",
    "\n",
    "features_to_analyse = [x for x in quantitative_features_list if x in golden_features_list]\n",
    "\n",
    "features_to_analyse.append('hate_crime_incident_count')\n",
    "\n",
    "print(features_to_analyse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  plot quantitative features of our dataframe and how they relate to the hate_crime_incident_count\n",
    "fig, ax = plt.subplots(round(len(features_to_analyse) / 3), 3, figsize = (30, 30))\n",
    "\n",
    "for i, ax in enumerate(fig.axes):\n",
    "    if i < len(features_to_analyse) - 1:\n",
    "        sns.regplot(x=features_to_analyse[i],y='hate_crime_incident_count', data=fbi_data[features_to_analyse], ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "``Build model``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Train a Model"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "copyright"
   ],
   "name": "03 Linear Regression with Tensorflow.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
